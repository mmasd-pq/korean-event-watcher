# enhanced_debug.py - ã‚ˆã‚Šè©³ç´°ãªHTMLæ§‹é€ è§£æ
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re

def analyze_site_deep(url, site_name):
    """ã‚µã‚¤ãƒˆã®HTMLæ§‹é€ ã‚’è©³ç´°ã«åˆ†æ"""
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ è©³ç´°åˆ†æ: {site_name}")
    print(f"URL: {url}")
    print(f"{'='*80}")
    
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/115.0.0.0 Safari/537.36"
        )
    }
    
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        resp.raise_for_status()
        
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # 1. å…¨ä½“çš„ãªãƒªãƒ³ã‚¯åˆ†æ
        all_links = soup.find_all("a", href=True)
        print(f"ğŸ“Š ç·ãƒªãƒ³ã‚¯æ•°: {len(all_links)}")
        
        # 2. ã‚¤ãƒ™ãƒ³ãƒˆé–¢é€£ã£ã½ã„ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        event_patterns = [
            r'event', r'ã‚¤ãƒ™ãƒ³ãƒˆ', r'ì´ë²¤íŠ¸',
            r'popup', r'íŒì—…', r'ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—',
            r'news', r'ãƒ‹ãƒ¥ãƒ¼ã‚¹', r'ë‰´ìŠ¤',
            r'notice', r'ãŠçŸ¥ã‚‰ã›', r'ê³µì§€',
            r'detail', r'è©³ç´°', r'ìƒì„¸'
        ]
        
        print(f"\nğŸ¯ ã‚¤ãƒ™ãƒ³ãƒˆé–¢é€£ãƒªãƒ³ã‚¯ã®åˆ†æ:")
        event_links = []
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # ã‚¤ãƒ™ãƒ³ãƒˆé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            combined_text = (href + ' ' + text).lower()
            if any(re.search(pattern, combined_text, re.IGNORECASE) for pattern in event_patterns):
                event_links.append({
                    'href': href,
                    'text': text[:100],
                    'parent_class': link.parent.get('class') if link.parent else None,
                    'parent_tag': link.parent.name if link.parent else None,
                    'link_class': link.get('class')
                })
        
        print(f"   è¦‹ã¤ã‹ã£ãŸã‚¤ãƒ™ãƒ³ãƒˆé–¢é€£ãƒªãƒ³ã‚¯: {len(event_links)}å€‹")
        
        # 3. ä¸Šä½10å€‹ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒªãƒ³ã‚¯ã‚’è©³ç´°è¡¨ç¤º
        for i, link_info in enumerate(event_links[:10]):
            print(f"\n--- ã‚¤ãƒ™ãƒ³ãƒˆãƒªãƒ³ã‚¯ {i+1} ---")
            print(f"href: {link_info['href']}")
            print(f"text: {link_info['text']}")
            print(f"parent: <{link_info['parent_tag']} class='{link_info['parent_class']}'>")
            print(f"link class: {link_info['link_class']}")
        
        # 4. æ¨å¥¨ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ç”Ÿæˆ
        print(f"\nğŸ’¡ æ¨å¥¨ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼:")
        generate_selectors_from_analysis(event_links, soup)
        
        # 5. ä¸€èˆ¬çš„ãªæ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èª¿æŸ»
        print(f"\nğŸ—ï¸ HTMLæ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³:")
        analyze_common_patterns(soup)
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

def generate_selectors_from_analysis(event_links, soup):
    """åˆ†æçµæœã‹ã‚‰é©åˆ‡ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ææ¡ˆ"""
    if not event_links:
        print("   âŒ ã‚¤ãƒ™ãƒ³ãƒˆé–¢é€£ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # è¦ªè¦ç´ ã®ã‚¯ãƒ©ã‚¹åˆ†æ
    parent_classes = {}
    link_classes = {}
    href_patterns = {}
    
    for link in event_links:
        # è¦ªã®ã‚¯ãƒ©ã‚¹
        if link['parent_class']:
            for cls in link['parent_class']:
                parent_classes[cls] = parent_classes.get(cls, 0) + 1
        
        # ãƒªãƒ³ã‚¯ã®ã‚¯ãƒ©ã‚¹
        if link['link_class']:
            for cls in link['link_class']:
                link_classes[cls] = link_classes.get(cls, 0) + 1
        
        # hrefãƒ‘ã‚¿ãƒ¼ãƒ³
        href = link['href']
        for pattern in ['/event', '/detail', '/popup', '/news']:
            if pattern in href.lower():
                href_patterns[pattern] = href_patterns.get(pattern, 0) + 1
    
    print("   ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼:")
    for cls, count in sorted(parent_classes.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"     .{cls} a â†’ {count}å€‹ã®ãƒªãƒ³ã‚¯")
    
    for cls, count in sorted(link_classes.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"     a.{cls} â†’ {count}å€‹ã®ãƒªãƒ³ã‚¯")
    
    print("   hrefãƒ™ãƒ¼ã‚¹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼:")
    for pattern, count in sorted(href_patterns.items(), key=lambda x: x[1], reverse=True):
        print(f"     a[href*='{pattern}'] â†’ {count}å€‹ã®ãƒªãƒ³ã‚¯")

def analyze_common_patterns(soup):
    """ã‚ˆãã‚ã‚‹æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
    # ãƒªã‚¹ãƒˆæ§‹é€ 
    lists = soup.find_all(['ul', 'ol'])
    print(f"   ãƒªã‚¹ãƒˆè¦ç´ : {len(lists)}å€‹")
    
    for i, lst in enumerate(lists[:3]):
        items = lst.find_all('li')
        if items:
            print(f"     ãƒªã‚¹ãƒˆ{i+1}: {len(items)}å€‹ã®ã‚¢ã‚¤ãƒ†ãƒ ")
            # æœ€åˆã®ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
            first_item = items[0]
            link = first_item.find('a')
            if link:
                print(f"       ä¾‹: {link.get('href', 'no-href')} - {link.get_text(strip=True)[:50]}")
    
    # ã‚«ãƒ¼ãƒ‰/ã‚¢ã‚¤ãƒ†ãƒ æ§‹é€ 
    card_patterns = ['.card', '.item', '.box', '.post', '.article', '.event']
    for pattern in card_patterns:
        elements = soup.select(pattern)
        if elements:
            print(f"   {pattern}: {len(elements)}å€‹")
            # æœ€åˆã®è¦ç´ ã®ãƒªãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
            first = elements[0]
            link = first.find('a')
            if link:
                print(f"     ä¾‹: {link.get('href', 'no-href')} - {link.get_text(strip=True)[:50]}")

def analyze_specific_sites():
    """å•é¡Œã®ã‚ã‚‹ã‚µã‚¤ãƒˆã‚’é›†ä¸­åˆ†æ"""
    problem_sites = [
        {
            "name": "MEGABOX ã‚¤ãƒ™ãƒ³ãƒˆ", 
            "url": "https://www.megabox.co.kr/event"
        },
        {
            "name": "CGV ã‚¤ãƒ™ãƒ³ãƒˆ",
            "url": "https://www.cgv.co.kr/culture-event/event/"
        },
        {
            "name": "LOTTE CINEMA ã‚¤ãƒ™ãƒ³ãƒˆ", 
            "url": "https://www.lottecinema.co.kr/LCWS/Event/EventList.aspx"
        }
    ]
    
    for site in problem_sites:
        analyze_site_deep(site["url"], site["name"])
        
        print("\n" + "="*40)
        input("æ¬¡ã®ã‚µã‚¤ãƒˆã«é€²ã‚€ã«ã¯ Enter ã‚’æŠ¼ã—ã¦ãã ã•ã„...")

if __name__ == "__main__":
    print("ğŸ”¬ éŸ“å›½ã‚¤ãƒ™ãƒ³ãƒˆã‚¦ã‚©ãƒƒãƒãƒ£ãƒ¼ - è©³ç´°HTMLæ§‹é€ è§£æãƒ„ãƒ¼ãƒ«")
    print("å•é¡Œã®ã‚ã‚‹ã‚µã‚¤ãƒˆã‚’è©³ç´°ã«åˆ†æã—ã¾ã™...")
    analyze_specific_sites()
